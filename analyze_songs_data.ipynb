{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Data Lake on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.hadoop:hadoop-aws:2.7.0 pyspark-shell\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure that your AWS credentials are loaded as env vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "#Normally this file should be in ~/.aws/credentials\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "#os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "#os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "KEY=config.get('AWS','AWS_ACCESS_KEY_ID')\n",
    "SECRET= config.get('AWS','AWS_SECRET_ACCESS_KEY')\n",
    "#print(KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create spark session with hadoop-aws package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "hadoop_conf=sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "hadoop_conf.set(\"fs.s3a.awsAccessKeyId\", KEY)\n",
    "hadoop_conf.set(\"fs.s3a.awsSecretAccessKey\", SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to song data file\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                     )\n",
    "\n",
    "sampleDbBucket =  s3.Bucket(\"udacity-dend\")\n",
    "\n",
    "for obj in sampleDbBucket.objects.filter(Prefix=\"song_data/\"):\n",
    "    print(obj)\n",
    "    #key = obj.key\n",
    "    #body = obj.get()['Body'].read()\n",
    "    #print(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read song data file\n",
    "df=spark.read.json(\"s3a://udacity-dend/song_data/A/A/A/*.json\")\n",
    "df.printSchema()\n",
    "df.select('song_id','title','artist_id','year','duration').show()\n",
    "songs_table = df\n",
    "columns_to_drop = ['artist_name','artist_location','artist_latitude', 'artist_longitude','num_songs']\n",
    "songs_table = songs_table.drop(*columns_to_drop)\n",
    "songs_table.select('song_id','title','artist_id','year','duration').dropDuplicates().collect()  \n",
    "# write songs table to parquet files partitioned by year and artist\n",
    "songs_table.write.format(\"parquet\").mode(\"overwrite\").save(\"data/output/songs.parquet\")\n",
    "#.partitionBy(\"year\",\"artist_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.select('artist_id','artist_name','artist_location','artist_latitude', 'artist_longitude').dropDuplicates()\n",
    "artists_table = df.select('artist_id','artist_name','artist_location','artist_latitude', 'artist_longitude')\n",
    "#columns_to_drop = ['duration', 'num_songs','song_id','title','year']\n",
    "#artists_table = artists_table.drop(*columns_to_drop)\n",
    "#artists_table.printSchema()\n",
    "artists_table.select('artist_id','artist_name','artist_location','artist_latitude', 'artist_longitude').dropDuplicates().collect()\n",
    "artists_table.write.parquet(\"data/output/artists.parquet\",mode='overwrite',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to song data file\n",
    "#song_data = \n",
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                     )\n",
    "\n",
    "sampleDbBucket =  s3.Bucket(\"udacity-dend\")\n",
    "\n",
    "for obj in sampleDbBucket.objects.filter(Prefix=\"log_data/\"):\n",
    "    print(obj)    \n",
    "    \n",
    "for obj in sampleDbBucket.objects.filter(Prefix=\"log_data/2018/11/2018-11-01-events.json\"):   \n",
    "    key = obj.key\n",
    "    body = obj.get()['Body'].read()\n",
    "    print(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.json(\"s3a://udacity-dend/log_data/2018/11/*.json\")\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by actions for song plays\n",
    "df1=df.filter(df.method=='GET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.count()\n",
    "df1.printSchema()\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_table = df.select('userId','firstName','lastName','gender','level')\n",
    "\n",
    "\n",
    "users_table.select('userId','firstName','lastName','gender','level').dropDuplicates().collect()\n",
    "users_table=users_table.withColumnRenamed('userId','user_id') \\\n",
    "                            .withColumnRenamed('firstName','first_name') \\\n",
    "                            .withColumnRenamed('lastName','last_name')                          \n",
    "# write users table to parquet files    \n",
    "users_table.write.parquet(\"data/output/users.parquet\",mode='overwrite',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Duplicates in Columns\n",
    "import pyspark.sql.functions as udf\n",
    "users_table.agg(\n",
    "    udf.count('user_id').alias('count'),\n",
    "    udf.countDistinct('user_id').alias('distinct count')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicate rows using subset method\n",
    "users_table=users_table.dropDuplicates(subset=[c for c in users_table.columns if c!='user_id'])\n",
    "users_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will give in every rows how many columns has misisng values e.g. For the User_Id='' is having 4 columns out of which 3 cloumns doesn't have any value. \n",
    "users_table.rdd.map(\n",
    "    lambda row:(row['user_id'],sum([c==None for c in row]))\n",
    ").collect()\n",
    "\n",
    "#users_table.where(col(\"user_id\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+--------------------+-------------+\n",
      "|user_id_missing|  first_name_missing|   last_name_missing|      gender_missing|level_missing|\n",
      "+---------------+--------------------+--------------------+--------------------+-------------+\n",
      "|            0.0|0.018867924528301883|0.018867924528301883|0.018867924528301883|          0.0|\n",
      "+---------------+--------------------+--------------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Percentage of missing values in each columns\n",
    "users_table.agg(*[\n",
    "    (1-(udf.count(c) /udf.count('*'))).alias(c+'_missing')\n",
    "    for c in users_table.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- dayofmonth: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- weekofyear: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- date_format: string (nullable = true)\n",
      "\n",
      "+-------------------+----+-----+----------+----+----------+-------+-----------+\n",
      "|                 ts|year|month|dayofmonth|hour|weekofyear|weekday|date_format|\n",
      "+-------------------+----+-----+----------+----+----------+-------+-----------+\n",
      "|2018-11-15 00:30:26|2018|   11|        15|   0|        46|      5| 11/15/2018|\n",
      "|2018-11-15 00:41:21|2018|   11|        15|   0|        46|      5| 11/15/2018|\n",
      "|2018-11-15 00:45:41|2018|   11|        15|   0|        46|      5| 11/15/2018|\n",
      "|2018-11-15 01:57:51|2018|   11|        15|   1|        46|      5| 11/15/2018|\n",
      "|2018-11-15 03:29:37|2018|   11|        15|   3|        46|      5| 11/15/2018|\n",
      "+-------------------+----+-----+----------+----+----------+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create timestamp column from original timestamp column\n",
    "#get_timestamp = udf()  year, month, dayofmonth, hour, weekofyear, date_format\n",
    "import  pyspark.sql.functions as udf\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import pandas as pd\n",
    "\n",
    "time_table = df\n",
    "columns_to_drop = ['artist','auth','itemInSession','length','location','method', \\\n",
    "                   'page','registration','sessionId','song','status','userAgent', \\\n",
    "                   'userId','firstName','lastName','gender','level']\n",
    "time_table = time_table.drop(*columns_to_drop)\n",
    "time_table.printSchema()\n",
    "time_table.select('ts').dropDuplicates().collect()\n",
    "time_table = time_table.withColumn(\"ts\",udf.to_timestamp(udf.from_unixtime(udf.col(\"ts\")/1000))) \\\n",
    "           .withColumn(\"year\", udf.year(\"ts\")) \\\n",
    "           .withColumn(\"month\", udf.month(\"ts\")) \\\n",
    "           .withColumn(\"dayofmonth\", udf.dayofmonth(\"ts\")) \\\n",
    "           .withColumn(\"hour\", udf.hour(\"ts\")) \\\n",
    "           .withColumn(\"weekofyear\", udf.weekofyear(\"ts\")) \\\n",
    "           .withColumn(\"weekday\", udf.dayofweek(\"ts\")) \\\n",
    "           .withColumn(\"date_format\", udf.date_format(\"ts\",'MM/dd/yyy')) \n",
    "time_table.printSchema()\n",
    "time_table['ts','year','month','dayofmonth','hour','weekofyear','weekday','date_format'].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "PyArrow >= 0.8.0 must be installed; however, it was not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-ffe2a5e663b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Use pandas_udf to define a Pandas UDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mpandas_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPandasUDFType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSCALAR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# Input/output are both a pandas.Series of doubles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m_create_udf\u001b[0;34m(f, returnType, evalType)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_minimum_pyarrow_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mrequire_minimum_pyarrow_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0margspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_argspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mrequire_minimum_pyarrow_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhave_arrow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         raise ImportError(\"PyArrow >= %s must be installed; however, \"\n\u001b[0;32m--> 149\u001b[0;31m                           \"it was not found.\" % minimum_pyarrow_version)\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminimum_pyarrow_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         raise ImportError(\"PyArrow >= %s must be installed; however, \"\n",
      "\u001b[0;31mImportError\u001b[0m: PyArrow >= 0.8.0 must be installed; however, it was not found."
     ]
    }
   ],
   "source": [
    "# create timestamp column from original timestamp column\n",
    "#get_timestamp = udf()  year, month, dayofmonth, hour, weekofyear, date_format\n",
    "import  pyspark.sql.functions as udf\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "# Use pandas_udf to define a Pandas UDF\n",
    "@pandas_udf('timestamp', PandasUDFType.SCALAR)\n",
    "# Input/output are both a pandas.Series of doubles\n",
    "\n",
    "def millisToTimeStamp(t):\n",
    "    return pd.to_datetime(t,unit='ms')\n",
    "\n",
    "time_table = df['ts']\n",
    "time_table.printSchema()\n",
    "time_table=time_table.withColumn('ts',millisToTimeStamp(time_table['ts']))\n",
    "time_table.printSchema()\n",
    "time_table.select('ts').dropDuplicates().collect()\n",
    "\n",
    "time_table = time_table.withColumn(\"ts\",udf.to_timestamp(udf.from_unixtime(udf.col(\"ts\")/1000))) \\\n",
    "           .withColumn(\"year\", udf.year(\"ts\")) \\\n",
    "           .withColumn(\"month\", udf.month(\"ts\")) \\\n",
    "           .withColumn(\"dayofmonth\", udf.dayofmonth(\"ts\")) \\\n",
    "           .withColumn(\"hour\", udf.hour(\"ts\")) \\\n",
    "           .withColumn(\"weekofyear\", udf.weekofyear(\"ts\")) \\\n",
    "           .withColumn(\"weekday\", udf.dayofweek(\"ts\")) \\\n",
    "           .withColumn(\"date_format\", udf.date_format(\"ts\",'MM/dd/yyy')) \n",
    "time_table.printSchema()\n",
    "time_table['ts','year','month','dayofmonth','hour','weekofyear','weekday','date_format'].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df=spark.read.json(\"s3a://udacity-dend/song_data/A/A/A/*.json\")\n",
    "song_df.printSchema()\n",
    "#song_df.show(5)\n",
    "\n",
    "songplays_table = df\n",
    "songplays_table.printSchema()\n",
    "#songplays_table.show(5)\n",
    "\n",
    "songsplay_final=songplays_table.join(song_df,songplays_table.song==song_df.title,'inner') \\\n",
    "        .drop('auth','itemInSession','method','page','registration','status','firstName','lastName','gender','artist_latitude', \\\n",
    "             'artist_location','artist_longitude','artist_name','duration','num_songs','year','song','artist','title')  \n",
    "\n",
    "songsplay_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer schema, fix header and separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"s3a://udacity-dend/pagila/payment/payment.csv\",sep=\";\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix the data yourself "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pyspark.sql.functions as F\n",
    "dfPayment = df.withColumn(\"payment_date\", F.to_timestamp(\"payment_date\"))\n",
    "dfPayment.printSchema()\n",
    "dfPayment.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPayment = dfPayment.withColumn(\"month\", F.month(\"payment_date\"))\n",
    "dfPayment.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer aggregate revenue per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfPayment.createOrReplaceTempView(\"payment\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT month, sum(amount) as revenue\n",
    "    FROM payment\n",
    "    GROUP by month\n",
    "    order by revenue desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date\n",
    "paymentSchema = R([\n",
    "    Fld(\"payment_id\",Int()),\n",
    "    Fld(\"customer_id\",Int()),\n",
    "    Fld(\"staff_id\",Int()),\n",
    "    Fld(\"rental_id\",Int()),\n",
    "    Fld(\"amount\",Dbl()),\n",
    "    Fld(\"payment_date\",Date()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPaymentWithSchema = spark.read.csv(\"s3a://udacity-dend/pagila/payment/payment.csv\",sep=\";\", schema=paymentSchema, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPaymentWithSchema.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPaymentWithSchema.createOrReplaceTempView(\"payment\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT month(payment_date) as m, sum(amount) as revenue\n",
    "    FROM payment\n",
    "    GROUP by m\n",
    "    order by revenue desc\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
