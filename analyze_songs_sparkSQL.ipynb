{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Data Lake on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure that your AWS credentials are loaded as env vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "#Normally this file should be in ~/.aws/credentials\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "#os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "#os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "KEY=config.get('AWS','AWS_ACCESS_KEY_ID')\n",
    "SECRET= config.get('AWS','AWS_SECRET_ACCESS_KEY')\n",
    "#print(KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create spark session with hadoop-aws package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "hadoop_conf=sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "hadoop_conf.set(\"fs.s3a.awsAccessKeyId\", KEY)\n",
    "hadoop_conf.set(\"fs.s3a.awsSecretAccessKey\", SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to song data file\n",
    "#song_data = \n",
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                     )\n",
    "\n",
    "sampleDbBucket =  s3.Bucket(\"udacity-dend\")\n",
    "\n",
    "for obj in sampleDbBucket.objects.filter(Prefix=\"song_data/\"):\n",
    "    print(obj)\n",
    "    #key = obj.key\n",
    "    #body = obj.get()['Body'].read()\n",
    "    #print(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read song data file\n",
    "df=spark.read.json(\"s3a://udacity-dend/song_data/A/A/A/*.json\")\n",
    "df.printSchema()\n",
    "df.select('song_id','title','artist_id','year','duration').show()\n",
    "songs_table = df\n",
    "columns_to_drop = ['artist_name','artist_location','artist_latitude', 'artist_longitude','num_songs']\n",
    "songs_table = songs_table.drop(*columns_to_drop)\n",
    "songs_table.select('song_id','title','artist_id','year','duration').dropDuplicates().collect()  \n",
    "# write songs table to parquet files partitioned by year and artist\n",
    "songs_table.write.format(\"parquet\").mode(\"overwrite\").save(\"data/output/songs.parquet\")\n",
    "#.partitionBy(\"year\",\"artist_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.select('artist_id','artist_name','artist_location','artist_latitude', 'artist_longitude').dropDuplicates()\n",
    "artists_table = df.select('artist_id','artist_name','artist_location','artist_latitude', 'artist_longitude')\n",
    "#columns_to_drop = ['duration', 'num_songs','song_id','title','year']\n",
    "#artists_table = artists_table.drop(*columns_to_drop)\n",
    "#artists_table.printSchema()\n",
    "artists_table.select('artist_id','artist_name','artist_location','artist_latitude', 'artist_longitude').dropDuplicates().collect()\n",
    "artists_table.write.parquet(\"data/output/artists.parquet\",mode='overwrite',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to song data file\n",
    "#song_data = \n",
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                     )\n",
    "\n",
    "sampleDbBucket =  s3.Bucket(\"udacity-dend\")\n",
    "\n",
    "for obj in sampleDbBucket.objects.filter(Prefix=\"log_data/\"):\n",
    "    print(obj)    \n",
    "    \n",
    "for obj in sampleDbBucket.objects.filter(Prefix=\"log_data/2018/11/2018-11-01-events.json\"):   \n",
    "    key = obj.key\n",
    "    body = obj.get()['Body'].read()\n",
    "    print(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.json(\"s3a://udacity-dend/log_data/2018/11/*.json\")\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by actions for song plays\n",
    "df1=df.filter(df.method=='GET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.count()\n",
    "df1.printSchema()\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_table = df.select('userId','firstName','lastName','gender','level')\n",
    "\n",
    "\n",
    "users_table.select('userId','firstName','lastName','gender','level').dropDuplicates().collect()\n",
    "users_table=users_table.withColumnRenamed('userId','user_id') \\\n",
    "                            .withColumnRenamed('firstName','first_name') \\\n",
    "                            .withColumnRenamed('lastName','last_name')                          \n",
    "# write users table to parquet files    \n",
    "users_table.write.parquet(\"data/output/users.parquet\",mode='overwrite',compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Duplicates in Columns\n",
    "import pyspark.sql.functions as udf\n",
    "users_table.agg(\n",
    "    udf.count('user_id').alias('count'),\n",
    "    udf.countDistinct('user_id').alias('distinct count')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicate rows using subset method\n",
    "users_table=users_table.dropDuplicates(subset=[c for c in users_table.columns if c!='user_id'])\n",
    "users_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will give in every rows how many columns has misisng values e.g. For the User_Id='' is having 4 columns out of which 3 cloumns doesn't have any value. \n",
    "users_table.rdd.map(\n",
    "    lambda row:(row['user_id'],sum([c==None for c in row]))\n",
    ").collect()\n",
    "\n",
    "#users_table.where(col(\"user_id\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+--------------------+-------------+\n",
      "|user_id_missing|  first_name_missing|   last_name_missing|      gender_missing|level_missing|\n",
      "+---------------+--------------------+--------------------+--------------------+-------------+\n",
      "|            0.0|0.018867924528301883|0.018867924528301883|0.018867924528301883|          0.0|\n",
      "+---------------+--------------------+--------------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Percentage of missing values in each columns\n",
    "users_table.agg(*[\n",
    "    (1-(udf.count(c) /udf.count('*'))).alias(c+'_missing')\n",
    "    for c in users_table.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- dayofmonth: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- weekofyear: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- date_format: string (nullable = true)\n",
      "\n",
      "+-------------------+----+-----+----------+----+----------+-------+-----------+\n",
      "|                 ts|year|month|dayofmonth|hour|weekofyear|weekday|date_format|\n",
      "+-------------------+----+-----+----------+----+----------+-------+-----------+\n",
      "|2018-11-15 00:30:26|2018|   11|        15|   0|        46|      5| 11/15/2018|\n",
      "|2018-11-15 00:41:21|2018|   11|        15|   0|        46|      5| 11/15/2018|\n",
      "|2018-11-15 00:45:41|2018|   11|        15|   0|        46|      5| 11/15/2018|\n",
      "|2018-11-15 01:57:51|2018|   11|        15|   1|        46|      5| 11/15/2018|\n",
      "|2018-11-15 03:29:37|2018|   11|        15|   3|        46|      5| 11/15/2018|\n",
      "+-------------------+----+-----+----------+----+----------+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create timestamp column from original timestamp column\n",
    "#get_timestamp = udf()  year, month, dayofmonth, hour, weekofyear, date_format\n",
    "import  pyspark.sql.functions as udf\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "import pandas as pd\n",
    "\n",
    "time_table = df\n",
    "columns_to_drop = ['artist','auth','itemInSession','length','location','method', \\\n",
    "                   'page','registration','sessionId','song','status','userAgent', \\\n",
    "                   'userId','firstName','lastName','gender','level']\n",
    "time_table = time_table.drop(*columns_to_drop)\n",
    "time_table.printSchema()\n",
    "time_table.select('ts').dropDuplicates().collect()\n",
    "time_table = time_table.withColumn(\"ts\",udf.to_timestamp(udf.from_unixtime(udf.col(\"ts\")/1000))) \\\n",
    "           .withColumn(\"year\", udf.year(\"ts\")) \\\n",
    "           .withColumn(\"month\", udf.month(\"ts\")) \\\n",
    "           .withColumn(\"dayofmonth\", udf.dayofmonth(\"ts\")) \\\n",
    "           .withColumn(\"hour\", udf.hour(\"ts\")) \\\n",
    "           .withColumn(\"weekofyear\", udf.weekofyear(\"ts\")) \\\n",
    "           .withColumn(\"weekday\", udf.dayofweek(\"ts\")) \\\n",
    "           .withColumn(\"date_format\", udf.date_format(\"ts\",'MM/dd/yyy')) \n",
    "time_table.printSchema()\n",
    "time_table['ts','year','month','dayofmonth','hour','weekofyear','weekday','date_format'].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df=spark.read.json(\"s3a://udacity-dend/song_data/A/A/A/*.json\")\n",
    "song_df.printSchema()\n",
    "#song_df.show(5)\n",
    "\n",
    "songplays_table = df\n",
    "songplays_table.printSchema()\n",
    "#songplays_table.show(5)\n",
    "\n",
    "songsplay_final=songplays_table.join(song_df,songplays_table.song==song_df.title,'inner') \\\n",
    "        .drop('auth','itemInSession','method','page','registration','status','firstName','lastName','gender','artist_latitude', \\\n",
    "             'artist_location','artist_longitude','artist_name','duration','num_songs','year','song','artist','title')  \n",
    "\n",
    "songsplay_final.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
